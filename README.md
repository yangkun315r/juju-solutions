# Apache Hadoop with Spark and Zeppelin

This bundle is a 7 node cluster designed to scale out. Built around Apache
Hadoop components, it contains the following units:

* 1 NameNode (HDFS)
* 1 ResourceManager (YARN)
* 3 Slaves (DataNode and NodeManager)
* 1 Spark
  - 1 Plugin (colocated on the Spark unit)
  - 1 Zeppelin (colocated on the Spark unit)


## Usage

Deploy this bundle using juju-quickstart:

    juju quickstart apache-hadoop-spark-zeppelin

See `juju quickstart --help` for deployment options, including machine
constraints and how to deploy a locally modified version of the
apache-hadoop-spark-zeppelin bundle.yaml.


## Testing the deployment

### Smoke test HDFS admin functionality

Once the deployment is complete and the cluster is running, ssh to the HDFS
Master unit:

    juju ssh namenode/0

As the `ubuntu` user, create a temporary directory on the Hadoop file system.
The steps below verify HDFS functionality:

    hdfs dfs -mkdir -p /tmp/hdfs-test
    hdfs dfs -chmod -R 777 /tmp/hdfs-test
    hdfs dfs -ls /tmp # verify the newly created hdfs-test subdirectory exists
    hdfs dfs -rm -R /tmp/hdfs-test
    hdfs dfs -ls /tmp # verify the hdfs-test subdirectory has been removed
    exit


### Smoke test YARN and MapReduce

Run the `terasort.sh` script from the Zeppelin unit to generate and sort data. The
steps below verify that Zeppelin is communicating with the cluster via the plugin
and that YARN and MapReduce are working as expected:

    juju ssh zeppelin/0
    ~/terasort.sh
    exit


### Smoke test HDFS functionality from user space

From the Zeppelin unit, delete the MapReduce output previously generated by the
`terasort.sh` script:

    juju ssh zeppelin/0
    hdfs dfs -rm -R /user/ubuntu/tera_demo_out
    exit


### Smoke test Spark

SSH to the Zeppelin unit and run the SparkPi demo as follows:

    juju ssh zeppelin/0
    ~/sparkpi.sh
    exit


### Access the Zeppelin web interface

Access the Apache Zeppelin web interface at
`http://{spark_unit_ip_address}:9090`. The IP address can be found by running
`juju status spark/0 | grep public-address`.


## Scale Out Usage

This bundle was designed to scale out. To increase the amount of Compute
Slaves, you can add units to the compute-slave service. To add one unit:

    juju add-unit slave

Or you can add multiple units at once:

    juju add-unit -n4 slave


## Contact Information

- <bigdata-dev@lists.launchpad.net>


## Help

- [Juju mailing list](https://lists.ubuntu.com/mailman/listinfo/juju)
- [Juju community](https://jujucharms.com/community)
